MSB7103:Bio-Unix & Shell Scripting (5 CU). Course Description. Unix is an operating system commonly used in bioinformatics analysis and programming. Many tools used in bioinformatics work best when run in a Unix-based, command line environment. As such, working in a Unix environment is a fundamental skill needed by all bioinformatics and genomics researchers especially the ability to work productively in a Unix-based, command-line environment. This course will provide students with a working knowledge of computer systems, programming and interfacing with other software essential to a career in bioinformatics. Course Objectives: At the end of this course, the student should be able to: Explain the Unix Shell and command-line environment. Outline procedures of obtaining and installing command-line utilities. Discuss data processing and visualization techniques. Perform shell scripting and develop pipelines or workflows using Shell scripting. Course Learning Outcomes: By the end of this course, students should be able to: Comfortably operate within the Unix Shell and command-line environment. Identify and appropriately obtain, install, utilize and troubleshoot command-line utilities. Develop analytical pipelines and workflows using Shell scripting. Process data sets in a pipeline of linked steps and checking results using a visualization tool. Indicative Content: Mode of Delivery: This course will be delivered as a participatory, hands-on practical sessions, and active learning.  Learning will be facilitated through lectures, tutorial videos, Individual and group work comprising of assignments and prescribed reading. Mode of Assessment: Group Assignments                                             		10%. Individual Assignments                                                 	20%. Course Assessment Tests					10%. Final examination     						60%. Reading List/References: Learning the bash Shell, (1998), 2nd Edition - O'Reilly Press. UNIX for Dummies Quick Reference, (1998), 4th Edition by Margaret Levine Young, John R.Levine. Computational Biology, (2004):Unix/Linux, Data Processing and Programming by Röbbe Wünschiers. UNIX. ● Unix is an operating system (OS) developed in the late 1960s at AT&T's Bell Labs. ● Used in various applications, including servers, workstations, and embedded systems. ● Unix systems have both the Command Line Interface (CLI) and Graphical User Interface (GUI). ● There are many different varieties of UNIX and the most popular ones are Linux, Solaris, and MacOS. Parts of a Unix OS. Figure121.1.3.biounix_and_shell_scripting_image_001.jpg. ● Kernel - The core/heart of the OS. Manages processes, memory, software and hardware. ● Shell - The shell serves as the interface between the user and kernel. Enables a user to instruct the system to perform tasks. ● There are different types of shells like the bourne shell (sh), Bash (Bourne Again SHell), korn shell (ksh), C shell (csh), and the z shell (zsh). ● Tools and Apps - Performs speciﬁc tasks for the user. LINUX. ● A Unix-like operating system developed by Linus Torvalds in 1991. ● Linux is free and open source whereas some of the original derivatives of Unix are not. ● It is a fast, secure, multi-user and multi-tasking OS. ● Linux has multiple distributions (distros) like Ubuntu, Debian, CentOS, Fedora, and others. Ubuntu. ● One of the most common and popular Linux distributions. ● Easy and convenient to use for most beginners. ● Download and Installation instructions. (https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview ). Figure122.1.3.biounix_and_shell_scripting_image_002.jpg. Windows Subsystem for Linux (WSL). ● WSL enables you to install a Linux distribution (Ubuntu, Kali, Debian, etc.,) and use Linux applications directly on windows. ● This eliminates the traditional methods like using virtual machines and dual-booting. ● Installation instructions. (https://learn.microsoft.com/en-us/windows/wsl/install ). The terminal. ● The terminal is a wrapper program that runs the shell. ● The most common shell is bash and it comes as the default shell in most Linux distributions. What is Linux? Why use Linux?. Linux is an operating system, like Microsoft Windows or Mac OSX (and lately Mac OS 11). An operating system (OS) is a computer program which manages how you connect to various pieces of hardware (screen, keyboard, mouse etc.) and also manages your files and allows you to run software applications. An OS will generally come with a graphical user interface (GUI) through which a user interacts with it. Each GUI looks a little different. It is worth noting that some of the terminology used varies from OS to OS so, for example, a folder in MS Windows is equivalent to a directory in Linux. We will begin with a little history. The term Linux actually refers to a family of open source, Unix-like operating systems based on the Linux kernel, which was developed by Linus Torvalds, and first released in 1991. Unix itself refers to a family of multitasking, multi-user operating systems all derived from the original AT&T Unix, which was developed in the late 1970s. In practice, although Unix strictly refers to a commercial OS and Linux refers to a freely available, open-source OS, the two words tend to be used interchangeably. Linux now has many variants, all based around the same original kernel. Some are developed for specific purposes (e.g., a very minimal distribution which uses little disk space), others have more general, wide-ranging purposes. Some of the more widely used versions of Linux include Ubuntu, CentOS, Redhat Enterprise Linux, Debian and Linux Mint. Whilst they vary in terms of GUI and some aspects of their core software (e.g., the method of installing software applications), they have the same core commands in common. Linux is powerful, stable and robust and, with sufficient resources, allows dozens of users to run programs simultaneously on the same computer. This ability to scale upwards makes it the preferred operating system for large scale scientific computation. Linux is an operating system that can be used in everything from Android phones to desktop PCs through to super computers. Around the home, items such as smart TVs and newer, smart washing machines use Linux. But, why is Linux key to biology and bioinformatics? Increasingly, the output of lab-based biology exists as large text files, for example the data files from sequencing machines. Linux has a range of powerful and flexible commands which can be used to edit and manipulate these files. Importantly, these commands can be combined to effectively produce new commands. Whilst any Linux OS contains hundreds of commands, a user will generally find that they perform most basic analyses using, maybe, a dozen of them. In addition to the OS commands, the fact that Linux is freely available and incredibly flexible has meant that a large community of bioinformaticians and computer programmers has developed using it as their main OS and developing software primarily for use with Linux. NOTE: Please note that for the purpose of this course only, we use Linux and Unix interchangeably. The Terminal and the Command Line. For the bioinformatician, the things which make Linux most powerful are the terminal and, within it, the command line. Most Linux distributions come with an array of graphical user interface (GUI) based software included in them. There will generally be at least one web browser, a word processor, spreadsheet software and software for delivering presentations amongst them. In addition, in the field of bioinformatics, there are many graphically based programs available for Linux, which can be used for both data analysis and visualisation. However, for the bioinformatician, the things which make Linux most powerful are the terminal and, within it, the command line. The Linux terminal is simply software which allows a user to interface with the computer by typing in text-based commands. The Mac OS has a terminal which is the direct equivalent of the one found in Linux. Older Windows versions had the MS-DOS prompt. Windows 10 now has the Windows Terminal, in addition to similar text-based interfaces such as PowerShell and Command Prompt. Figure123.1.3.biounix_and_shell_scripting_image_003.jpg. The command line is the current line within the terminal which can be used to input text commands. Each command is run through a shell, a program which sends commands to the computer for it to interpret. Common shells include bash, zsh, tcsh/csh and ksh. In this course, we will focus on the bash shell. The basic commands tend to be the same across all shells but some of the more advanced commands may vary in syntax a little between them. The command line allows users to run a vast range of commands and includes facilities to run successive commands as one single operation, with the output of one command fed directly into the next. It is also the starting point for running most bioinformatics software. File system hierarchy. ● Unix-like operating system such as Linux and its distributions have a hierarchical directory structure, just like in Windows OS. ● This means the directories are organised in a tree-like pattern and each directory may contain ﬁles and other directories. ● A directory is usually referred to as a folder in Windows Operating Systems. Figure124.1.3.biounix_and_shell_scripting_image_004.jpg. ● /: The root directory is the ﬁrst directory and stores all the other directories. ● /bin: Contains essential user binaries. Contains common command line user commands and other binary executable ﬁles. ● /boot: Contains all the ﬁles required for the boot process. ● /home: Contains the users’ directories where the users can create and manipulate ﬁles. ● /mnt: Mount points for removable storage devices. File system hierarchy - detailed. Source:  https://www. 2daygeek.com/linux-directory-structure-ﬁle-system-hierarchy/. Figure125.1.3.biounix_and_shell_scripting_image_005.jpg. Directory names are separated by a / character, both when written and typed in the terminal. The top level of the hierarchy is referred to as the root directory. This will typically contain some key files used by the system and the start of a hierarchy of directories. It is owned by the root user and cannot generally be written to by standard users. Think of the root user as the admin user on Windows or Mac OS but with far greater powers. The root directory is represented just by a / character when typed. The directories may vary a little but include places where essential system software and libraries, default configuration files for all user accounts, file representations of devices and media attached to the system, software installed for general use and user home directories are stored. Each user will always have a home directory. This is where their files and directories are stored. An individual user will usually be the only person able to write to their home directory unless they set permissions so others can do so. We will learn about file permissions later in the week. Linux Command Syntax. ● Command -[options] [arguments]. ● Command --[options] [arguments]. ● Options modify the behaviour of the command whereas the arguments are the items upon which the command acts e.g., ﬁles, directories, and data. ● Options are preceded by a hyphen (-) when written in short format or (--) when written in their full form. Options can be combined by listing them successively after the minus sign. Commands. ● pwd - Print name of current working directory. ● ls - List directory contents. ● cd - Change directory. Navigating the ﬁle system - pwd. ● At any one point, a user is within a given directory and may have directories above or subdirectories below. ● The directory you are currently in is called the current working directory. ● When you launched the terminal for the ﬁrst time, the home directory will be your current working directory. ● Every user account has its own home directory where a regular user is allowed to write ﬁles. Navigating the ﬁle system - ls. ● ls command will list the contents of your current working directory or any other directory of your choice. Common ls options. ● ls -l   display output in a long format. ● ls -1   lists one ﬁle/directory per line. ● ls -r display contents in reverse order. ● ls -a   lists all the contents of a given directory. ● ls -h display ﬁle size in human readable format. ● -ls -t display contents by modiﬁcation time. ● Note: To explore other options type ls -- help to show the help page or man ls to show the manual page.  man and help can be used with most commands. Navigating the ﬁle system - cd. ● The cd command enables a user to move from the current directory into another directory. ● The cd command is typed and followed by the path to the desired directory. ● The path is the route you take to get to your desired directory and may be absolute or relative. ● If a path is not speciﬁed, this command will default to the home directory of the.  user which can also be detonated by the tilde (~). Absolute path. ● The absolute path starts from the root (/) directory up to the desired directory or ﬁle e.g. , cd /usr/bin/. Relative path. ● The relative path starts from your current working directory up to the desired directory or ﬁle.  e.g. , cd bash2023/malaria/ﬁle.txt assuming bash2023 is my current working directory. ● When using the cd command, the notation .  also refers to the current working directory and .. refers to the parent directory of current working directory. ● cd - will change directory to the previous directory. Manipulating ﬁles and directories. ● There are a series of commands that can be used to manipulate ﬁles and directories. ● These commands can be used to create, copy, move, remove/delete, ﬁles and directories among other functions. ● Mastering these commands can improve your productivity when working within a unix-like environment. Make/create directories - mkdir. ● mkdir can be used to create a single or multiple directories. ● To create a single directory - mkdir dir1,. ● To create multiple directories - mkdir dir1 dir2 dir3. Common mkdir option. ● -p, --parents - No error if existing. Create an empty ﬁle - touch. ● touch is one of the commonest ways of creating a single or multiple or multiple empty ﬁles. ● To create a single empty ﬁle - touch ﬁle1. ● To create multiple empty ﬁles - touch ﬁle1 ﬁle2 ﬁle3. • touch command options. • -a: change the access time only. • -c: if the file does not exist, do not create it. • -d: update the access and modification times. • -m: change the modification time only. • -r: use the access and modification times of the file. • -t: creates a file using a specified time. Create and edit a text ﬁle - nano. ● nano is a command line editor that can be used to create as well as edit the contents of a text ﬁle. ● It is one of the most user-friendly text-ﬁle editors on the command-line. It’s a WYSIWYG editor: “what you see is what you get”. What you type directly goes into the text input. ● Other powerful command line editors are vi, vim, and emacs. These require some learning. ● To create and edit the contents of a text ﬁle - nano ﬁle1. Text Editing and Writing to Files. Text editing for Linux environment. GNU nano is a text editor for Unix-like environments or other operating environments using a command line/ terminal interface.  Nano can also be used with pointing devices, such as a mouse to activate toolbar functions and position a cursor. Other file editing tools are available such as Emacs, vi and pico, but for the purposes of this example we’ll use the easy-to use nano text editor. To demonstrate creating a new file in nano, open a terminal and type the following example. This example creates a new file called fruit.txt.  nano fruit.txt. The nano editor will launch and it’s now possible to type freely within the window. Figure126.1.3.biounix_and_shell_scripting_image_006.jpg. Now enter the following data and hit the enter key after each item. Orange, Pear, Apple, Banana, Grape, Satsuma, Melon. Figure127.1.3.biounix_and_shell_scripting_image_007.jpg. Select control-x to Exit the nano editor and type y. Figure128.1.3.biounix_and_shell_scripting_image_008.jpg. A prompt to save the file will appear. Press the enter key to save and this will write the file to disk. Figure129.1.3.biounix_and_shell_scripting_image_009.jpg. This will return you back to the terminal window. Typing ls will show you the new fruit.txt file you have created. Type.  ls. This will display the fruit.txt file that you have created. Nano has a wide range of editing capabilities, which are highlighted in the link below. Concatenate ﬁles - cat. ● cat command reads and outputs the contents of one or more ﬁles. ● cat ﬁle1 - output the content of ﬁle1. ● cat ﬁle1 ﬁle2 - output the content of ﬁle1 and ﬁle2. ● Command is useful for displaying the contents of short ﬁles. Copy ﬁles and directories - cp. ● The cp command will copy a ﬁle or directory. ● cp ﬁle1 ﬁle2 - copy ﬁle1 into ﬁle2. If ﬁle2 already exists, then it is overwritten.  with the contents of ﬁle1 and if ﬁle2 doesn't exist then it will be created. ● cp ﬁle1 dir1 - copy ﬁle1 into dir1.  dir1 must already exist.  common cp options. ● -r, -R, --recursive - recursively copy directories and their contents. ● -i, --interactive - ask the user for conﬁrmation before overwriting an existing ﬁle. ● Copying one directory into another is done by specifying the recursive option  (-r).  i.e cp -r dir1 dir. ● This implies that a directory as well as its ﬁles and subdirectories will be copied. ● Copying a ﬁle using the interactive [-i] option will ensure that a user is prompted before overwriting useful contents of an already existing ﬁle. Move and rename ﬁles and directories - mv. ● mv ﬁle1 ﬁle2 - If ﬁle2 exists then it is overwritten with ﬁle1 and if ﬁle2 doesn't exist then it will be created. This is the same as renaming a ﬁle. ● mv ﬁle1 ﬁle2 dir1 - move ﬁle1 and ﬁle2 into an existing dir1. ● mv dir1 dir2 - move dir1 in dir2 and if dir2 doesn't exist it will be created. This is the same as renaming a directory. Common mv option. ● -i, --interactive - ask the user for conﬁrmation before overwriting an existing ﬁle. Remove/delete ﬁles and directories - rm. ● rm ﬁle1 - remove a ﬁle. ● rm ﬁle1 ﬁle2 - remove multiple ﬁles. Common rm options. ● -r, --recursive recursively remove/delete directories and their contents. ● -i, --interactive ask the user for conﬁrmation before deleting an existing ﬁle. ● -f, --force ignore non-existent ﬁle and do not ask the user for conﬁrmation. This.  overrides the [-i] option. ● rm  -r dir1 -  recursively remove/delete a directory and its content. ● rm -r dir1 dir2 - recursively remove/delete directories and their content. ● rmdir command can be used to remove an empty directory(s).  e.g rmdir dir1. Note. A user should be very careful when using the rm command because once a ﬁle or directory is deleted, it cannot be recovered. Symbolic Links and their Use. A symlink is a symbolic Linux/ UNIX link that points to another file or folder on your computer, or a connected file system. This is similar to a Windows shortcut. Symlinks can take two forms: Soft links are similar to shortcuts, and can point to another file or directory in any file system. Hard links are also shortcuts for files and folders, but a hard link cannot be created for a folder or file in a different file system. These can be created in the following way on Linux and Mac operating systems.  ln -s /<path to file/folder to be linked> <path of the link to be created>.  ln is the link command. The -s flag specifies that the link should be soft. -s can also be entered as -symbolic. Please note, ln command creates hard links by default. The next argument is path to the file (or folder) that you want to link. (That is, the file or folder you want to create a shortcut for). The last argument is the path to the link itself i.e., Where you would like the shortcut to be placed. This could be the Desktop for example (N. B.  name= Home folder name): Linux.  ln -s /home/name/Documents/MyFolder /home/name/Desktop/MyFolder. For folders or files with special characters, like spaces, then use quotes around the paths.  ln -s “/home/name/Documents/My Folder” “/home/name/Desktop/MyFolder”. Mac.  ln -s /Users/name/Documents/MyFolder /Users/name/Desktop/MyFolder. For folders or files with special characters, like spaces, then use quotes around the paths.  ln -s “/Users/name/Documents/My Folder” “/Users/name/Desktop/My Folder”. If you find that you can’t make a symlink due to permission issues, then prefix the commands above with sudo. The sudo command is an abbreviation of “super user do”, and is quite a dangerous command in the hands of the inexperienced, as it allows programs to be executed as the root user/ administrator.  e.g.  sudo ln -s /home/name/Documents/MyFolder /home/name/Desktop/MyFolder. Exercise: Here is an exercise to demonstrate the use of symlinks. First create two folders in your home folder called WS1 and WS2.  mkdir WS1 WS2. Change the directory to the WS1 folder and create a new file within this folder called colours.txt:  cd WS1.  touch colours.txt. Next, use the change directory command to move to the WS2 folder.  cd . /WS2. Now we’re going to create a symlink in this directory (WS2) to the colours.txt file we created in the WS1 folder.  ln -s /<full path>/WS1/colours.txt colours.txt. Please note, the <full path> will be /home/<whatever your home folder is called>/WS1. You can check this by typing the pwd (Print Working Directory) command, as this will show you the full pathway information. So, to create the symlink we use the following expression. Here myname equals the name of your home folder. Please note: This will be case sensitive.  ln -s /home/myname/WS1/colours.txt colours.txt. This will create a symlink in your WS2 folder called colours.txt, which points to the colours.txt file in your WS1 folder. Type the following list command to see the symlink information for colours.txt.  ls -l. This will show an output similar to this.  lrwxr-xr-x 1 myname 1753 44 10 Sep 08:20 colours.txt -> /home/myname/WS1/colours.txt. The colours.txt symlink in the WS2 folder is now pointing to the colours.txt file in the WS1 folder. Redirect output of a command into a ﬁle - >. ● > (greater than sign) is used to redirect the output of a command into a ﬁle. ● Example.  ls dir1 > ﬁle.txt. ● In the above example, the ls command lists the contents of dir1 and > redirects the output into the ﬁle named ﬁle.txt. ● If ﬁle.txt doesn’t exist then it will be created and if it already exists then its contents will be overwritten. Append output of a command into a ﬁle - >>. ● > > (2 greater than signs) is used to append the output of a command to the end of a ﬁle. ● Example.  ls dir1 >> ﬁle.txt. ● In the above example, the ls command lists the contents of dir1 and > > appends the output into the ﬁle called ﬁle.txt. ● This is best used when ﬁle.txt already exists and has content that you do not want to overwrite. Output the ﬁrst few lines of a ﬁle - head. ● Sometimes you don’t want an output displaying all the lines a ﬁle. ● You can use the head to return only the ﬁrst few lines of the ﬁle (default is the ﬁrst 10 lines). ● Example.  head ﬁle.txt.  common head option. ● -n - this option returns the ﬁrst n lines of the ﬁle e.g. , head -n 5 ﬁle.txt. Output the last few lines of a ﬁle - tail. ● Similarly, sometimes you are only interested in seeing the last few lines of a ﬁle. ● You can use the tail command to return only the last few lines of the ﬁle (default is last 10 lines). ● Example.  tail ﬁle.txt. Common tail option. ● -n - this option returns the last n lines of the ﬁle e.g. , tail -n 5 ﬁle.txt. View ﬁle contents and navigate through it- more. ● The more command enables you to view the contents of a ﬁle and navigate through it. ● Example.  more ﬁle.txt. ● You can navigate through the ﬁle line by line using the “Enter” key on the keyboard or navigate through page by page using the “spacebar”. ● When done, you can use the “q” key on your keyboard to quit the navigation screen. View ﬁle contents and navigate through it- less and more. ● The less and more commands are “more or less” the same with respect to their purpose. ● However, minor differences can be observed with the way they are used. ● For example, the arrow keys (up and down) will work with the less command but not the more command. ● less may therefore have some options that are absent in more and vice versa. The pipe operator - |. ● The pipe operator [ | ] is used to direct the standard output of one command as the standard input of another command. ● syntax- command 1 | command 2. Figure130.1.3.biounix_and_shell_scripting_image_010.jpg. ● Example. cat ﬁle1.txt | head. Sort lines of text - sort. ● The sort command helps you to arrange the lines of ﬁles in a particular order. ● By default, the sorting is done in ascending order. common sort options. ● -r, --reverse - reverse the order of sorting. ● -u, --unique - output the unique lines of text. ● -n, sort numerically. Sort on a speciﬁc column (n°4): sort –k 4 <ﬁlename>. Sort based on a tab separator: sort -t $'\t’ <ﬁlename>. Report or omit repeated lines - uniq. ● uniq takes a sorted ﬁle as input and by default removes any duplicates lines. ● This command is therefore often used in conjunction with sort. ● Example. sort ﬁle.txt | uniq. ● In the above example, the sort command ﬁrst arranges the lines of text in the ﬁle.txt in ascending order and then the output is passed as input to uniq via the pipe operator. ● Note - using uniq without ﬁrst sorting your ﬁle may not yield any useful output. Common uniq options. ● -d, --repeated - only print duplicate/repeated lines. ● -u, --unique - only print unique lines. ● -c, -output each line with its number of repeats. Question. What difference do you observe when you use uniq with and without the -u option?. What are sort and uniq?. Ordering and manipulating data in Linux-based text files can be carried out using the sort and uniq utilities. The sort command orders a list of items both alphabetically and numerically, whereas the uniq command removes adjacent duplicate lines in a list. Let’s work through an example together, please play along!. First, using nano editor (or any other text editor that you might prefer) create a text file with the following content (one fruit per line): orange pear apple banana grape satsuma melon pomegranate banana grape. Name it fruit.txt. How to use sort. The sort command accepts input from a text-based file and outputs its results to the screen.  sort  fruit.txt. The sort results can also be output into another text file.  sort fruit.txt > sorted_fruit.txt. You can reverse the order of the sort with the -r option.  sort -r fruit.txt. Scrambling the order of lines is also possible with the -R option.  sort -R fruit.txt. The -f option forces the sort to ignore the case of a letter when ordering lines.  sort -f fruit.txt. The -s option stabilises the sort by outputting identical lines in the same order as they appeared in the original file.  sort -s fruit.txt. Duplicate lines can be removed with the -u option.  sort -u fruit.txt. How to use uniq. The uniq command accepts input from a text-based file and removes any repeated lines, only if they are adjacent to each other. That’s why it’s used in conjunction with sort to remove non-adjacent lines.  sort fruit.txt | uniq. Case differences can be ignored when dropping duplicate adjacent lines, using the -i option.  sort fruit.txt | uniq -i. Combining -i with the -c option for uniq, counts the number of times a line occurs in a file.  sort fruit.txt | uniq -ic. Using the -d option with -i inverts the behaviour of uniq and only prints the duplicated lines.  sort fruit.txt | uniq -id. It can be helpful to pipe this output into the input of another uniq command.  sort fruit.txt | uniq -id | uniq -f 1. This will list the unique fruit names from fruit.txt (uniq -id). This output is then piped to ‘uniq -f 1’ which ignores the first field in each input line when doing comparisons (-f 1 equals the first field). For more on sort and uniq visit . Print newline, word, and byte counts for each ﬁle - wc. ● The word count (wc) command displays the number of lines, words, and bytes in ﬁles e.g. , wc ﬁle.txt. Common wc options. ● -l, --lines - print the number of lines. ● -w, --words - print word counts. ● -c, --bytes - print byte counts. ● -m, --chars - print the character counts. When you use either wc -m or wc -c, you will get the count of characters in the specified file. The only difference is that -m is used to count characters explicitly, while -c is equivalent and also counts characters. The choice between them is more a matter of historical conventions rather than functional differences. Print lines matching a pattern- grep. ● global regular expression print (grep) is used to search for and by default return the lines of a ﬁle that matches a speciﬁed pattern. ● Syntax - grep pattern ﬁlename e.g. , grep “lion” ﬁle.txt assuming that the ﬁle.txt.  contains lines with the word lion. ● The pattern can be a simple character, word, or complex patterns written using.  regular expressions.  common options. ● -v, --invert-match - select non matching lines. ● -i, --ignore-case - ignore the case. ● -c, --count - count only the lines where the pattern matches. ● -n, --line-number - print line number with output lines matching the pattern. ● -w, --word-regexp - force the pattern to match only whole words. One thing we often need to do is check whether a particular file or set of files contains a string of characters. This process is known as pattern matching. Using the command line, this is done with a command called grep. The name derives from the old Unix editor, ed, and stands for “globally search for a regular expression and print matching lines”. A regular expression is a string of characters defining a search pattern. Regular expressions can become incredibly complex, involving any number of special characters, short cuts and groupings so, for now, we will deal only with searches involving plain text. At its most simple, grep is run like this on a single file:  grep -F querystring filename.txt. As output, it displays a line by line list of all lines containing the text querystring. Please note that if the query string includes space characters, quotes should be placed at either end of it. Additionally, the -F option signifies that we are searching for fixed strings rather than regular expressions. This can be omitted by advanced users searching for complicated regular expressions. Grep has many command line options to refine searches and change the output format. Run the following to see all of the available options:  man grep. Extract specific fields froma file - cut. Syntax: cut <options> <ﬁlename>. Important cut options are: ● -d (field delimiter), ● -f (field specifier). Example: extract ﬁelds 2 and 3 from a ﬁle having ‘space’ as a separator; cut -d’‘ -f2,3 <ﬁlename>. Read from Stdin and Output to Stdout and Files - tee. ● tee reads the standard input and writes it to the standard output and to one or more ﬁles. ● This enables the user to store the content of intermediate ﬁles during. Processing. Figure131.1.3.biounix_and_shell_scripting_image_011.jpg.  image source: geeksforgeeks.org. ● Example.  cat list.txt | tee ﬁle1.txt | wc -l. ● In the above example, tee takes the input from the cat command, copies it to a ﬁle and also writes it to the standard output to be used by the wc command.  common tee option. ● -a, --append   - append to a ﬁle, do not overwrite e.g. ,.  example- cat list.txt | tee -a ﬁle1.txt | wc -l. Compare two sorted ﬁles line by line - comm. ● comm command can be used to compare the contents of 2 sorted ﬁles line by line. ● It returns the lines that are unique to each ﬁle as well as the lines common to both ﬁles. ● Example.  comm ﬁle1.sorted.txt ﬁle2.sorted.txt. ● The output from the above example will have 3 ﬁelds (columns). The ﬁrst showing lines unique to ﬁle1.sorted.txt, the second showing lines unique to ﬁle2.sorted.txt, and the third showing lines common to both ﬁles.  common comm options. ● -1    - suppress column 1 (lines unique to the ﬁrst ﬁle). ● -2   - suppress column 2 (lines unique to second ﬁle). ● -3   - suppress column 3 (lines that appear in both ﬁles). Join lines of two ﬁles on a common ﬁeld - join. ● join enables a user to merge two ﬁles based on a common ﬁeld. ● Example join ﬁle1.txt ﬁle2.txt. ● In the above example, the lines in ﬁle2.txt will be added to those of ﬁle1.txt based on the ﬁrst ﬁeld (by default). ● For the join command to work, the two ﬁles must share a common ﬁeld (key ﬁeld) that will be used in the merger.  common join options. ● -i, --ignore-case - ignore differences in case when comparing ﬁelds. ● -1 -specify the ﬁeld of ﬁle 1 on which to perform the merger. ● -2 -specify the ﬁeld of ﬁle 2 on which to perform the merger. Compare 2 input files and display the different entries - diff. ● diﬀ compares the contents of 2 ﬁles, outputs the diﬀerences. Default output: common lines not showed, only diﬀerent lines are indicated and shows what has been adde (a), deleted (d) or changed (c). syntax: diﬀ <options> <ﬁlename1> <ﬁlename2>. It can be used to highlight differences between 2 versions of the same file. Search for ﬁles in a directory hierarchy - ﬁnd. ● The ﬁnd command enables a user to locate ﬁles and directories. ● Syntax.  ﬁnd <directory path> <search parameter/option>.  common search parameters. ● -name (provide ﬁle name). ● -iname (provide ﬁle extension). ● -type (provide type, f -ﬁle, d -directory, -empty). ● -size (c -bytes, k -kilobytes, M -megabytes, G -gigabytes, + bigger than, -smaller than). Examples. ● ﬁnd . -name ﬁle1.txt - starting from the current directory, ﬁnd ﬁle1.txt. ● ﬁnd dir1 -iname “*.txt” - starting from dir1, ﬁnd all ﬁles with the .txt extension. ● ﬁnd dir2 -type f - starting from dir2, ﬁnd all ﬁles. ● ﬁnd . -size +10k - starting from the current directory, ﬁnd ﬁles bigger than 10kb. ● ﬁnd dir3 -size -10k - starting from dir3, ﬁnd ﬁles smaller than 10kb. ●  ﬁnd . -type f -empty - starting from the current directory, ﬁnd all empty ﬁles. Wildcards. ● These are special characters that help to specify groups of ﬁlenames. Common wildcards. Figure132.1.3.biounix_and_shell_scripting_image_012.jpg. Imagesource:  https://www.osmage.com/wildcards-linux-bash/. ● Common character classes. Figure133.1.3.biounix_and_shell_scripting_image_013.jpg. Imagesource: Examples. ● ls *.txt - list all ﬁles in the current directory with the .txt extension. ● rm ﬁle?.txt - remove all ﬁles like ﬁle1.txt, ﬁle2.txt, but not ﬁles like ﬁle12.txt ﬁle24.txt. ● cat ﬁle[123].txt  - display the contents of ﬁles ﬁle1.txt, ﬁle2.txt, or ﬁle3.txt. ● ls ﬁle[0-9].txt  - list all the ﬁles in that range from ﬁle1.txt, ﬁle2.txt, . ., ﬁle9.txt. The hyphen [-] represents the range. ● ls ﬁle[!12].txt  - list any other ﬁle but not ﬁles ﬁle1.txt and ﬁle2.txt. Examples with character classes. ● grep [[:alnum:]] ﬁle.txt matches lines containing at least one alphanumeric character, such as "abc123" or "Hello_world". ● grep [[:alpha:]] ﬁle.txt matches lines containing at least one alphabetic character, like "Hello" or "world". ● grep [[:digit:]] ﬁle.txt matches lines containing at least one digit, such as "1234" or "2023". Brace Expansion { }. ● Brace expansion enables a user to simplify repetitive tasks by creating multiple arguments for a command out of a single argument. ● This is based on patterns speciﬁed within curly braces { }. Examples. ● mkdir dir{1. 20}  -This will create 20 directories named dir1, dir2, …, to dir20. ● touch ﬁle{a.g}.txt  -This will create ﬁles named ﬁlea.txt, ﬁleb.txt, …, to ﬁleg.txt. ● cp ﬁle{1. 3}.txt dir1  -Copy ﬁle1.txt, ﬁle2.txt, and ﬁle3.txt to the directory dir1. ● The { . } used in the previous examples specify a range. Commas { , } can be used.  to specify speciﬁc characters or numbers. Examples. ● mkdir dir{1,10,15}  -This will create 3 directories named dir1, dir10, and dir15. ● touch ﬁle{x,y,z}.txt  -This will create ﬁles named ﬁlex.txt, ﬁley.txt, and ﬁlez.txt. ● cp ﬁle{1,3,7}.txt dir1  -Copy ﬁle1.txt, ﬁle3.txt, and ﬁle7.txt to the directory dir1. Question. What will using both { , } and { . } do? e.g touch ﬁle{a,b,c}{1. 3}.txt. Stream editor for ﬁltering and transforming text - sed. ● sed (stream editor) can be used to transform/edit the contents of a ﬁle(s). Substitute. ● Sed is commonly used to substitute/replace text within a ﬁle using the syntax. Example. ● sed ‘s/kilogram/kg/’ weights.txt     - The “s” is the substitution operation, the “/” are the delimiters, “kilogram” is the pattern being searched for, and “kg” is the replacement pattern in the ﬁle weights.txt. ● By default, if there are multiple occurrences of the speciﬁed pattern in the lines of a ﬁle, sed will only substitute the ﬁrst occurrence of that pattern in each line. ● In such a scenario, a user can specify which occurrence of the pattern should be substituted in the ﬁle lines by modifying the substitution operation. ● For example, to substitute the nth occurrence of a pattern in a line -  sed ‘s/kilogram/kilo/2’ weights.txt. ● 2 in the above example represents the second occurrence. ● In most cases, the goal is to substitute all the occurrences of the pattern in all lines of the ﬁle. The “g” ﬂag is added which means global replacement. ● Example- ‘s/kilogram/kilo/g’ weights.txt. ● The substitution of a pattern can also be restricted to the nth line of the ﬁle. ● Example- ‘2s/kilogram/kilo/’’ weights.txt. ● In the above example, the 2 before the “s” indicates that the substitution is only performed on the second line. ● The substitution can also be performed on a range of lines. ● Example- ‘2,4 s/kilogram/kilo/’’ weights.txt. ● In the above example, the substitution is performed from the 2nd to the 4th line. ● sed can also be used to print the nth line of a ﬁle. ● Example- sed ‘2p’ ﬁle.txt prints the 2nd line of a ﬁle. The “p” represents the print operation. ● By default, the required line as well as the entire ﬁle is printed resulting into a duplication of the desired line. ● To output only the desired line, the -n option is added to the print operation e.g sed -n ‘2p’ ﬁle.txt. ● sed can also print a range of lines e.g sed -n ‘2,4 p’ ﬁle.txt will print lines 2 to 4 of the ﬁle.txt. ● sed can also be used to print a line(s) that matches a pattern. ● Example-    sed -n ‘/pattern/p’ ﬁle.txt. ● Question. Write a command that can be used to substitute and thereafter print only the lines of the ﬁle with the substituted pattern. ● Similarly, sed can also delete the nth line from a ﬁle e.g. , sed ‘1d’ ﬁle.txt. ● In the above example, 1 is the line to be deleted and “d” is the delete operation. ● Deletion can also be done on a range of lines e.g. , sed ‘2,5 d’ ﬁle.txt   will delete lines 2 up to line 5. ● A line(s) containing a pattern can also be deleted e.g sed ‘/pattern/d’ ﬁle.txt. Save sed output. ● To save the output from using sed, a user can either redirect the output into a ﬁle using the “>” or use the -i option in sed which is to edit in place. ● Editing in place means to directly modify the content of the original ﬁle. Examples. ● sed ‘1d’ ﬁle.txt > ﬁle2.txt -delete the ﬁrst line of ﬁle.txt and redirect the output to a ﬁle named ﬁle2.txt. ● sed -i ‘1d’ ﬁle.txt -delete the ﬁrst line of ﬁle.txt but edit in place. Replacing one text string with another is something we often need to do. This could be because we’ve discovered a repeated typo or misspelling in a file or because we’ve decided one piece of text needs replacing with something which reads better. We could, of course, do this manually, going through a file, correcting each appearance of an error. However, this is both time consuming and prone to human error. Linux thus includes a command called sed which allows us to make such substitutions automatically. The name sed is an abbreviation of stream editor. The command is run in the following way:  sed 's/oldtext/newtext/' file.txt. This will replace the string oldtext with newtext, taking input from the file file.txt and by default write the result to standout output, which is your terminal. To replace writing to a new file use a redirect:  sed 's/oldtext/newtext/' file.txt > newfile.txt. This will output instead to a file called newfile.txt. To make changes in place in the input file, use the -i option:  sed -i  's/oldtext/newtext/' file.txt. Only do this when absolutely sure that the substitution you are making is correct. By default, sed will replace all instances of the old text string with the new text string. Pattern scanning and processing language - awk. ● awk is an entire scripting language that can also be used for text processing. ● awk is named from the ﬁrst letters of the names of its developers Aho, Weinberger, and Kernighan. ● The awk syntax is awk options 'selection _criteria { action }' input-ﬁle-name. Examples. ● awk ‘{print}’ ﬁle.txt    - This will print the lines of ﬁle.txt. ● awk ‘/pattern/ {print}’ ﬁle.txt     - This will print the line(s) of ﬁle.txt containing the speciﬁed pattern. Inbuilt awk variables. ● awk has a number of inbuilt variables that can be speciﬁed to obtain the desired result. ● NF - Number of ﬁelds (columns) present in the input ﬁle. Each ﬁeld is speciﬁed by the “$” followed by the ﬁeld number e.g. , $0, $1, $3 … etc. ● $0 means the complete record and hence it indicates all the ﬁelds in the ﬁle. Figure134.1.3.biounix_and_shell_scripting_image_014.jpg. ● $NF is also used to represent the last ﬁeld and $(NF-1) represents the second last ﬁeld. Examples. ● awk ‘{print NF}’ ﬁle.txt   - print only the number of ﬁelds in each line of the input ﬁle ﬁle.txt. ● awk ‘{print NF,$0}’  ﬁle.txt    - print the number of ﬁelds in each line of ﬁle.txt as well as the complete record. ● awk ‘{print $3,$4}’  ﬁle.txt   - print the 3rd and 4th ﬁelds of  ﬁle.txt. ● awk ‘$3==”bed” {print}’  ﬁle.txt   - print all the lines in ﬁle.txt in which the 3rd ﬁeld contains the word bed. ● awk ‘$2==”big” && $3==”bed” {print}’  ﬁle.txt  - print all the lines in ﬁle.txt in which the 2nd ﬁeld contains the word “big” and 3rd ﬁeld contains the word bed. ● Question. Which other logical operators can be used in this context?. ● FS -Field Separator. This contains the ﬁeld separator character used to divide the input ﬁelds of the ﬁle. Default FS is a white space (tab or blank space). ● OFS -Output Field Separator. This stores the output ﬁeld separator, which separates the ﬁelds of a ﬁle when awk prints them. Default is a blank space. ● When printing the ﬁelds of a ﬁle whose default ﬁeld separator is not a tab or blank space, the user will have to specify the FS. ● Likewise, when the user wants an output whose ﬁelds are not separated by a blank space, the user will have to specify the OFS. Examples. ● awk ‘{print $3, $4}’ FS=“,”  ﬁle.csv    -print the 3rd and 4th lines of ﬁle.csv. Specify that the FS is a comma [,]. The OFS will default to a blank space. ● awk ‘{print $3, $4}’ FS=“,”  OFS=”\t” ﬁle.csv  -print the 3rd and 4th lines of ﬁle.csv. Specify that the FS is a comma [,] and make the OFS a tab. ● awk -v FS=","  '{print $3,$4}'    -alternative to the ﬁrst example. ● awk -v FS="," -v OFS="\t" '{print $3,$4}' - alternative to the second example. ● The -v option indicates that a value is being assigned to the variable FS or OFS. ● NR -Number of Records. Each line of the input ﬁle is considered as a record hence the NR variable keeps a count of all the records in the input ﬁle. Examples. ● awk ‘{print NR}’ ﬁle.txt    -print the NR of ﬁle.txt. ● awk ‘{print NR, $0}’ ﬁle.txt    -print the NR and all the lines in ﬁle.txt. ● awk ‘NR==2 {print}’ ﬁle.txt   -print only the 2nd line in the ﬁle. ● awk ‘NR==2, NR== 4 {print}’ ﬁle.txt -print a range of lines from the 2nd to the 4th line. ● RS -Record Separator. This stores the record separator character which separates the lines of the input ﬁle. ● The RS is a newline (\n) by default for most ﬁles. ● ORS -Output Record Separator. This stores the output record separator character which separates the output lines of a ﬁle when awk prints. ● The ORS is a newline (\n) by default for most ﬁles. ● When the record separator is not a new line, then the user will have to specify it using the RS variable. Examples. ● awk ‘{print $2 $3}’ RS=“:”  ﬁle.txt   -print the 2nd and 3rd lines of ﬁle.txt. Specify that the RS is a colon [:]. The ORS will default to a new line. ● awk ‘{print $2, $3}’ RS=“:”  ORS=”\n\n” ﬁle.txt  -print the 2nd and 3rd lines of ﬁle.txt. Specify that the RS is a colon [:] and make the ORS 2 newlines. ● awk -v RS=":"  '{print $2,$3}'      -alternative to the ﬁrst example. ● awk -v RS="," -v ORS="\t" '{print $2,$3}' - alternative to the second example. ● The -v option indicates that a value is being assigned to the variable RS or ORS. ● BEGIN - Variable used before awk takes any action. ● END - Variable used after awk takes an action. ● Question. How do you use BEGIN and END with awk?. AWK is a programming language. It is named after its three developers: Alfred Aho, Peter J Weinberger and Brian KernIghan. It is particularly useful for processing text files and extracting data, particularly when a file is split into columns or delimited by a specific character (e.g.  a comma). AWK is a standard package in most Linux releases as well as Mac OSX. AWK is generally written in upper case, although the command itself is lower case. AWK can be used to write complex scripts and programs but, in this course, we will use it directly on the command line. It reads a file line by line and splits each line into columns according to a delimiter character. The default delimiter is a single space character. For the example commands given below, we will work with two files from the course data. These are Diamonds.csv, which contains comma separated values and Diamonds_fix.txt, which is delimited with the tab character. Both files contain the same data in 10 columns: carat, cut, color, clarity, depth, table, price, x, y and z. The last 3 (x, y and z) relate to the dimensions of the diamond in question. In order to follow the commands below, please change directory into the directory containing your course data. Before beginning, it may be useful to run.  man awk.  to see the options available for AWK and also see how some of the commonly used delimiter characters are viewed. The first command to run is:  awk -F”\t” ‘{print $1}’ Diamonds_fix.txt. This will print the value in the first column of the file Diamonds_fix.txt. A little explanation: The option -F”\t” tells AWK that the delimiter is tab, which is generally represented as \t on the command line. Each delimited column is represented by a $ symbol followed by a number. The number represents the column number so $1 is column1, $2 is column 2 etc. $0 prints the whole line. For each line of the file, AWK will do whatever command is contained in the curly brackets. In this case, we are asking it to print the value in the first column. You may wish to change this command so it runs on the file Diamonds.csv by changing the delimiter in the -F option. Like grep, AWK can be used to filter files based on a line by line basis, based on the text they contain. However, as AWK splits lines into columns according to the delimiter, more precision is available. AWK can print only lines which have a specific value in a specific column. For example:  awk -F”\t” ‘$2==”Ideal” {print $0}’ Diamonds_fix.txt. This prints only the lines of Diamonds_fix.txt in which column 2 (cut) contains the value “Ideal”. Some explanation: The == symbol means ‘is equal to’. This is a common convention in programming languages with a single equals symbol generally meaning set the value to be. The print command only happens when column 2 contains the value ‘Ideal’. Generally, AWK commands are made of two parts, a pattern, such as ‘$2==”Ideal”’, and an action, in braces, such as ‘{print $0}’. The pattern defines the lines to which the action is applied. We could actually miss the action in this example, as AWK, by default will print the full line as output. Likewise, as we saw in the first example, if the pattern is omitted, AWK will perform the action on every line. Patterns can be combined using the && symbol (for and) so a line is printed only if two or more conditions are met. For example:  awk -F”\t” ‘$2==”Ideal” && $4==”SI2”’ Diamonds_fix.txt. This command will print all lines in which column 2 has the value ‘Ideal’ and column 4 has the value ‘SI2’. Likewise, using the || symbol (for or), we can print if any one of two or more conditions is met. For example:  awk -F”\t” ‘$2==”Ideal” || $4==”SI2”’ Diamonds_fix.txt. This will print a line column 2 has the value ‘Ideal’ or column 4 has the value ‘SI2’. In addition to strings, awk can also filter on numeric values. For example:  awk -F”\t” ‘$1>1’ Diamonds_fix.txt. This will print all lines in which the first column has a value greater than 1. You will notice that the first line of header values is included in this. We can omit this simply by adding the condition ‘FNR>1’:  awk -F”\t” ‘FNR>1 && $1>1’ Diamonds_fix.txt. FNR represents the current line number so we’re asking AWK to print the line if this is greater than 1 (i.e.  omit line 1). [omd] Setting p to 1 is a way to set a flag. In AWK scripts, it's a common practice to use a flag variable to control certain behaviours or conditions in the script. In this specific command:  awk '/Mean/ {p=1; next} p && />>END_MODULE/ {exit} p {print $2}' fastqc_data.txt. /Mean/ {p=1; next}: When the pattern "Mean" is found in a line, it sets p to 1. The next statement skips processing the rest of the AWK script for this line and moves to the next line.  p && />>END_MODULE/ {exit}: If p is true (i.e. , it has been set to 1), and the line contains ">>END_MODULE," it exits the AWK script. This is used to stop processing once the desired section has been processed.  p {print $2}: If p is true, it prints the second field of the line. In summary, setting p to 1 serves as a marker or flag to indicate that the script is currently processing lines between the "Mean" pattern and the ">>END_MODULE" pattern. It helps control the flow of the script based on specific conditions. Additional resources - sed and awk.  sed and awk have multiple applications to the extent that entire books have been written on how to use them. ● sed & awk, 2nd Edition - Dale Dougherty, Arnold Robbins. ● sed, awk and Regular Expressions - Arnold Robbins. ● Learning AWK Programming - Shiwang Kalkhanda. ● Effective awk Programming - Arnold Robbins. ● The AWK Programming Language - Alfred V. Aho, Brian W. Kernighan, Peter J. Weinberger. What are command line options. An option follows the command, usually started with a minus sign, and then a single letter. This gives the command an instruction to do something slightly more detailed than just its basic purpose. For example, with ls, I can type ls space -l. Arguments are the items on which the command is to be run. We've already used arguments when we cd'd into the course_data directory in which course_data was the argument.  tab complete. Linux command which. This shows you where the location of a binary file is that's running a programme. While the greater than sign is used to output data into a file, the less than sign can be used to input data from a file. Through a demonstration of this, I'll use the file "theme.txt" and I'll use the Unix command "sort", which run at its most basic will sort the data alphabetically. I'll start by showing you "theme.txt. ". In computer operating systems, an environment is an area that contains information about the behaviour of programs and applications. Linux environment variables are used by applications to get information about the environment, and each environment variable is a variable with a name and an associated value. Every time the environment is configured a new shell session is created, and this can be used as a learning experience to see how changing Linux environment variables can change things like the appearance of the shell, create paths to executable files, keyboard layout settings, and defining the default home directory for example. Variables have the following format and by convention have upper case names. Though they are case-sensitive, so it is possible to have lower case names. Also, there is no space around the equals = symbol. KEY=value KEY=”Another value”. If you assign multiple values to a variable then separate them with a colon : character. KEY=value1:value2. There are two types of variables: Environment variables are system wide and are inherited by all system processes and shells. Shell variables only apply internally to the current shell instance. How do we use Linux commands to list and set environment variables?. This can be achieved by using the following commands.  env – This allows you to run another program in a custom environment without modifying the current one. When used without an argument it will print a list of the current environment variables.  printenv – This prints all the specified environment variables. For example, to display the value of the HOME environment variable type:  printenv HOME. This will print out the path of the currently logged in user. /home/manager. You can also pass more than one argument to the printenv command, in the form:  printenv <argument1><argument 2>. Try this example.  printenv LANG PWD. This will produce an output similar to this one.  en_GB. UTF-8. Running printenv without any arguments will show a list of all the environment variables.  printenv. This will produce an extensive output similar to the truncated one below. CLUTTER_IM_MODULE=xim. LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*. Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*. 7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36: LESSCLOSE=/usr/bin/lesspipe %s %s. XDG_MENU_PREFIX=gnome-. LANG=en_GB. UTF-8. DISPLAY=:0. GNOME_SHELL_SESSION_MODE=ubuntu. COLORTERM=truecolor. USERNAME=manager. XDG_VTNR=1. SSH_AUTH_SOCK=/run/user/1000/keyring/ssh. XDG_SESSION_ID=1. USER=manager. DESKTOP_SESSION=ubuntu. QT4_IM_MODULE=xim. TEXTDOMAINDIR=/usr/share/locale/. GNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/f05913a2_61f0_4166_8fd7_9a75b1b624d0……. . Some of the more common environment variables are highlighted below.  set – This sets or unsets shell variables. If used without an argument then it will print a list of all variables, both shell and environment, and shell functions. Type set in a terminal and it will produce a long list of all the variables. BASH=/bin/bash. BASHOPTS=checkwinsize:cmdhist:complete_fullquote:expand_aliases:extglob:extquote:force_fignore:histappend:interactive_comments:progcomp:promptvars:sourcepath. BASH_ALIASES=(). BASH_ARGC=(). BASH_ARGV=(). BASH_CMDS=(). BASH_COMPLETION_VERSINFO=([0]="2" [1]="8"). BASH_LINENO=(). BASH_SOURCE=(). BASH_VERSINFO=([0]="4" [1]="4" [2]="20" [3]="1" [4]="release" [5]="x86_64-pc-linux-gnu"). BASH_VERSION='4. 4. 20(1)-release'. CLUTTER_IM_MODULE=xim. COLORTERM=truecolor. COLUMNS=80. DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus. DESKTOP_SESSION=ubuntu. DIRSTACK=(). DISPLAY=:0. EUID=1000. GDMSESSION=ubuntu……….  unset – This deletes shell and environment variables.  export – This command sets environment variables. Examples. Below are a few examples highlighting the difference between Environment and Shell variables. We’ll first create a Shell variable and then transform it into an Environment variable. Create a new variable called MY_VARIABLE and give a value of Linux_Variable. MY_VARIABLE=’Linux_Variable’. Verify the variable exists by typing:  echo $MY_VARIABLE. This will display: Linux_Variable. Use the printenv command to check whether MY_VARIABLE is an environment variable or not. If the output is empty then it is not an environment variable.  printenv MY_VARIABLE. This will display no output. Now we have our Shell variable, we can transform it into an Environment variable by typing:  export MY_VARIABLE. Check MY_VARIABLE exists by typing.  printenv MY_VARIABLE. This time, the variable will now be displayed as it is an Environment variable: Linux_Variable. Please note that variables created in this manner will only be available in the current session, and will be lost when you log out or open a new shell. In order to make the Environment variables persistent you’ll need to add them to specific Linux files. Edit the following file using the nano editor.  nano /etc/environment. Add MY_VARIABLE=value to a new line and Save. Define your value accordingly. This could be text or a number for example. Another useful method, if you are using Bash, is to declare your variable in the hidden .bashrc profile file.  nano ~/.bashrc. Add in a new line to the .bashrc file.  export MYVARIABLE=” value”. Save and update the changes to the .bashrc file by typing.  source ~/.bashrc. Hope you find this examples useful, do leave your comments in the comments section below. Please note that modifying the bashrc profile file can have disastrous effects on your system if you don’t know what you’re doing, and can result in you being unable to login. More advanced manipulation of the bashrc script is beyond the scope of this course, but if you’re interested in reading further then check out the following . BASH SCRIPTING. What is Bash/Shell Scripting?. Before we look at scripting, let’s take a step back and break down what a shell is and how it relates to you and your computer. What is a shell?. We can crudely break a computer down into two main components: the operating system (e.g. UNIX) and the hardware (e.g.  memory, graphics or CPU). Without hardware, there is very little that can be done with a computer. In order to communicate with this hardware, a piece of software, known as the operating system, is installed. Figure135.1.3.biounix_and_shell_scripting_image_015.jpg. The operating system is typically broken down into two parts: the user space and the kernel. The kernel has direct access to the hardware and is responsible for sensitive tasks such as managing resources. Meanwhile, the user space acts as an interface, translating the needs of the user into the system calls that get run by the kernel. So, now we get to the original question, “what is a shell”? A good analogy here is a hazelnut. At the core of a hazelnut is a soft, edible kernel, surrounded by the hardened shell to protect it. In computing, the shell is the generic name for an application or program which allows access to the system’s resources. You may also see this referred to as the command line interpreter or CLI. It acts as a wrapper, protecting the sensitive kernel inside. What is Bash?. There are many several types of shell available, each of which has a distinctive syntax. The first, the Thompson shell, was released in 1971 by Ken Thompson. However, this had many limitations leading Stephen Bourne to release his Bourne shell six years later, in 1977. Providing a more functional interaction with the operating system, the Bourne shell not only served as a CLI, but introduced variables, loops and control flows into shell scripts – useful concepts we’ll be introducing in more detail shortly!. Unfortunately, the most widely used shells at that time were not freely available. In order to bridge this divide, Brian Fox developed the Bourne-Again Shell (Bash) in 1988 for the GNU project. Bash is now the default shell on most Linux systems. Thus, understanding how to use it is a key part of any data scientist’s toolbox. Depending on your operating system, your default shell may not be Bash. To check which shell you are using, you can run the following command:  echo $SHELL. This will return the full path of your interpreter. If your interpreter is Bash you will see a path similar to: /bin/bash. Alternatively, you may also see the Z, C or TC shells which would have paths similar to one of the following: /bin/zsh. /bin/csh. /bin/tcsh. You can get a full list if the shells that are available on your system by running:  cat /etc/shells. To change the shell your computer uses temporarily (i.e.  in the current terminal) you can type the interpreter executable name. For example, to switch your shell to csh, you can enter:  csh. This is a good way to play with other shells without making a permanent change. To switch back to your existing shell, press Ctrl-D. If you like a particular shell and want to make the change permanent (i.e.  set in all future terminals by default), you can use the chsh command with the -s option (bypassing the editor) and the path to the interpreter you want to use.  chsh -s /bin/bash. Note: you may need to log out (close the terminal) and log back in again (open a new terminal) for this change to take effect. What is shell scripting?. Let’s say you want to know what proportion of lines in a file contain a particular phrase. First, you would need to determine the total number of lines in that file. Then, count the number of lines containing the phrase. Finally, you would need to perform a calculation to get the proportion of lines with that phrase. Simple, right?. Now let’s say you need to do this for 100 files, once for each file, and record the results…not so simple, eh?. You could take the extremely tedious approach of manually running the commands 100 times. But, how long would it be before you made a mistake? How would you track the output of all your commands? What if you needed to run the same thing again tomorrow?. No matter how careful you are, manual errors always creep in during these kinds of repetitive tasks. We have a simple path to salvation from this tedium… Bash/shell scripting!. Bash scripting is a powerful way to automate tasks which you need to execute on a computer. Put simply, a Bash script is just a collection of Bash commands which are kept in a text file. The shebang is present in any Bash script that you'll write. Using the shebang, we tell the operating system which shell interpreter we want to use, in this case, the Bash interpreter. There are several ways to invoke the Bash interpreter. The first is to use the absolute or full path to the interpreter in the shebang. Here we have two examples. The first looks for the interpreter in the bin directory (#!/bin/bash), and the second in the user bin directory (#!/usr/bin/env bash). However, if we define the absolute path to the interpreter this means we can have issues if we need to use the script on a system where the interpreter is located in a different place. To make our scripts portable we use the second shebang format, which will use the first bash executable that it finds in our environmental path. Adding Comments to Bash Scripts. Comments are one of the most important things to consider when you are writing your scripts. Making sure that you add comments throughout your scripts is just one of many good practices you should be keeping, no matter which programming language your scripts are written in. Comments help you keep track of what your script is doing. They are useful because they allow you to leave internal notes throughout your script to remind you of things like what the next command will do or its expected output. You’re essentially annotating your scripts. Why do I need to add comments to my script, I already know what it’s doing?. It’s not uncommon to write a script and then not come back to it for several months, at which point you will need to familiarize yourself with the code all over again. Comments simplify that process!. In Bash, we can identify comments using the # symbol. Every line that starts with the # sign is for our reference only. In Bash, anything after the # is not interpreted or executed. The only exception to this rule is the shebang which goes at the start of our script. Comments are a really useful way to describe complex functionality so that you, and other people, can find their way around your scripts. However, comments are only as good as the information they contain, keep them clear and keep them concise! • You can use the which command to locate the executable file associated with a given command. • which perl - /usr/bin/perl. • which bash - /bin/bash. Variables and Their Scope. Bash variables provide temporary storage for information. You can use them to store words/phrases (strings), decimals or integers. To assign a variable, we use the = symbol:  name="Victoria". BE CAREFUL! When you assign a value to a variable, there should not be any spaces on either side of the = symbol. When we want to access a variable, we need to use the $ symbol to reference it:  echo $name. Victoria. Because our variables may contain whitespace which gets interpreted by bash, it’s good practice to wrap the variable name in curly brackets and encase it in double quotes:  echo "${name}". We can add variables to our Bash scripts, for example: #!/usr/bin/env bash.  name="Victoria".  greeting="Good morning,".  echo "${greeting} ${name}". When you run this script, you should see: Good morning, Victoria. In Bash, variables don’t have to be declared. What we mean by this is that if you try to access a variable that doesn’t exist, you won’t see an error, just a blank value. For example, a script containing the following commands: #!/usr/bin/env bash.  var2="foo".  echo "Variable 1 value: ${var1}".  echo "Variable 2 value: ${var2}". Would return only the value of var2 because var1 had not been declared; Bash just ignored it. Variable 1 value: Variable 2 value: foo. Variable scope. There are two types of variables you need to be aware of: Local variables. Global variables. Local variables. Local variables are only accessible within the section of code in which they are declared. For example, if we declare a variable inside our Bash script, it will not be accessible outside of that script. Let’s reuse our earlier script: #!/usr/bin/env bash.  var2="foo".  echo "Variable 1 value: ${var1}".  echo "Variable 2 value: ${var2}". Now, we know when we run our script we can see the value of var2: Variable 1 value: Variable 2 value: foo. However, what happens if we call var2 outside of the script, directly in our terminal?.  echo "Variable 2 value: ${var2}". This time we get: Variable 2 value: As you can see, the scope of our variable was constrained to the script itself and its value is not accessible outside of that script. The same principle is true for functions, which we will look at later in the week. Global variables. In Week 1 we introduced you to global variables, also known as environment variables, that are available to all shells. You can recall global variables within your Bash scripts. Let’s print our current working directory by recalling the value of the global variable, PWD from within our script: #!/usr/bin/env bash.  echo "${PWD}". This will likely return a different path for you but, will look something like: /Users/tory. You can create your own global variables by using the export command. First let’s declare a variable that contains our name: MY_NAME="Victoria". Next, let’s create a script that tries to access this variable: #!/usr/bin/env bash.  echo "My name is: ${MY_NAME}". If we were to run this script, we wouldn’t see our name as the MY_NAME variable we created was only a local variable and therefore not accessible within our script. My name is: However, if we use the export command, we can declare MY_NAME as a global variable which is accessible in our script:  export MY_NAME="Victoria". Our script would then output: My name is: Victoria. Check Available Memory: Ensure that you have sufficient free memory available on your system. You can check the available memory using the free -h command.  ctr+z (pause) then kill%1 (to terminate the paused process). In the previous step we introduced you to Bash variables which can be used to hold temporary information. A variable holds a single value under a single name. By contrast, a Bash array can hold multiple values under a single name. You can initialise an array by assigning values that are separated by spaces in standard brackets. For example:  array=("value 1" "value 2" "value 3"). Remember, there should be no spaces on either side of the = symbol. Each value in an array is known as an element. Each element in an array is referenced by a numerical index. This index starts at 0. The syntax to access the first value in our array would be:  echo "${array[0]}". Notice that the index (0) is encased in square brackets. This will return the first value in our array:  value 1. We can return all of the values in our array by using the @ symbol:  echo "${array[@]}".  value 1 value 2 value 3. To count the number of elements in our array, we can prepend the array name with the # sign. In this case, we have 3 elements in our array:  echo "${#array[@]}".  fruits=("pineapple" "peach" "raspberry" "plum" "apple" "kiwi").  echo "Last fruit: ${fruits[5]}".  echo "Last fruit: ${fruits[@]:(-1)}". ${array[1]}: Accesses the second element of the array. ${array[-2]}: Accesses the second-to-last element of the array. ${array[*]} or ${array[@]}: Expands to all elements of the array. Though when elements contain spaces. "${array[*]}" treats the entire array as a single string,.  while "${array[@]}" treats each element as a separate word. Arrays allow you to store multiple values of the same type or different types under a single variable. You can easily iterate over all elements in an array using loops.  for item in "${fruit[@]}"; do. Arrays can be used to store the output of command executions files=($(ls)). If you want to create an array containing only files or only directories, you can use the find command along with the -type option to filter based on the type of the entry. Here's how you can do it: To create an array containing only files:  files_array=("$PWD"/**/*(. ) )  # Only regular files. And to create an array containing only directories:  directories_array=("$PWD"/**/*(/) )  # Only directories. Explanation: **/*: This recursive glob pattern matches all files and directories in the specified directory and its subdirectories. (. ): This is a qualifier that matches only regular files. (/): This is a qualifier that matches only directories. So, files_array will contain only regular files, and directories_array will contain only directories. Make sure to replace (*) and (/) with the appropriate pattern based on your requirements. A command line argument is a parameter that we can supply to our Bash script at execution. They allow a user to dynamically affect the actions your script will perform or the output it will generate. To pass an argument to your Bash script, your just need to write it after the name of your script: ./fruit.sh my_argument. In our Bash script, there are several reserved/pre-defined variables which we can use to recall the user-defined parameters. The first argument is stored in $1, the second in $2, the third in $3…and so on. We cannot use $0 as that references your Bash script itself. Let’s see how this works using an example script: #!/usr/bin/env bash.  echo "The first fruit is: $1".  echo "The second fruit is: $2".  echo "The third fruit is: $3". If we run our script and don’t give an argument, we will see no output for our pre-defined variables: Sometimes, we may want to access all of the arguments. We can do this using $@. As we’ve covered, the read command interactively captures user input, and so far, we’ve been using it in its simplest form. There are several options which are available for the read command to extend its functionality. To see them, we can type help read, which will give us the manual for the read command. There are two options I would like to introduce– minus p, which allows a message to be displayed before the input is read, and minus s, which will hide the user input as they’re typing it. While the user is entering their password, we will be hiding the text they’re typing but storing their response as a variable in our script, which we can then either return back to the user or use later on in our script if we wanted to. Let’s take a closer look at the minus p option first, which is used in both of our read commands. When we were prompting for user input previously, the prompt and the user’s input were on two separate lines. When we use the minus p option, this allows the user input to be entered on the same line as they see the message or prompt. We can also look at the minus s option. Here, we’re going to use this when we collect the password. The minus s option hides the user input as it’s being entered, so when we prompt asking them for their password, as the user is typing, the text that they are inputting won’t be shown in the terminal. But the text that they’ve entered isn’t lost. It will be stored in a variable in the background as part of our script. And we will then return it back to the user, just as an example. We’ll copy and paste our commands into that script, save it, and exit the editor. We’ll then update our permissions to make that script executable, and then we’ll execute our script, login.sh. First, you’ll see the prompt for your username. And you’ll be entering your response on the same line as that message. Next, you’ll see it prompt for your password. It looks like I’m doing nothing here, but while I’m typing, my input is hidden from the terminal. As you can see from the response, what I was typing was letmein as my password. This was stored as a variable, and then we’ve returned that back to you as an example in that message.  echo -e "\nHi ${username}, your password is ${password}" The -e option enables interpretation of backslash escapes, and \n adds a newline before the message. IF statements. Conditional statements come in many forms. The most basic form essentially says: IF our conditions are met, THEN execute the following code. We can write our if statements in several ways:  if [[ condition ]].  then.  command.  fi. So, if the expression within the square brackets returns true, then the code found between then and fi will be executed. If the expression returns false, the script will ignore (i.e.  not execute) any code which lies between then and fi. Notice that we end our if statement with fi which is if spelt backwards. An alternative format that you might come across uses a ‘;’ to allow then to be on the same line as the conditional expression:  if [[ condition ]] ; then.  command.  fi. You’ll notice that in these examples, we’ve used spacing to indicate the code which will run if the conditional expression returned true. This is known as indenting and, although there are no requirements for it in Bash, it is a good coding practice to follow for clean, readable code. A simple example you can try on the command line yourself is:  if [[ 1 == 1 ]] ; then.  echo hi.  fi. Here we are saying, if 1 is equal to 1 (1 == 1) then return ‘hi’ back to us. As this conditional expression is always true (as 1 is 1) then, it will always return ‘hi’. IF statements with AND/OR logic. We can use two (or more) conditional expressions with our if statement using the AND and/or OR conditions. For example, let’s say that we have a file and we want to check that it exists, is readable and that it isn’t empty. If our file meets these criteria, we want to print “File is good” and if not, print “File is bad”. First, let’s write our script: #!/usr/bin/env bash. # Set the path for our file.  file="file.txt". # Check whether file exists, is readable and has data.  if [[ -e ${file} ]] && [[ -r ${file} ]] && [[ -s ${file} ]].  then. # Execute this code if file meets those conditions.  echo "File is good".  else. # Execute this code if file does not meet those conditions.  echo "File is bad".  fi. Now, let’s run our script knowing that our file can’t meet the criteria as it doesn’t exist: ./script.sh. File is bad. Next, let’s create an empty file and try running our script again:  touch file.txt. ./script.sh. Again, our script returns “File is bad” as it hasn’t met all of our conditions. Finally, let’s add some data to our file and try again:  echo "hi" > file.txt. ./script.sh. Bingo! We have met all of the conditions and now our “File is good”. IF. ELSE statements. We can extend our conditional statement to have another clause by using an if.else statement. Here we are saying, IF our conditions are met, THEN execute the following commands. However, ELSE IF these conditions are not met, execute a different set of commands. The syntax for this looks like:  if [[ condition ]].  then.  command1.  else.  command2.  fi. IF. ELIF. ELSE statements. Sometimes, you may need to test more than one statement. The syntax for this looks like:  if [[ condition1 ]].  then.  command1.  elif [[ condition2 ]].  then.  command2.  else.  command3.  fi. It’s worth mentioning that you can have more than one elif clause in your if.elif.else statement. But, as we will soon discuss, there are more efficient ways of building that type of conditional statement. Using || (Logical OR):  if [ ! -e "$R1" ] || [ ! -e "$R2" ]. This condition is true if either $R1 does not exist or $R2 does not exist, or both. Using && (Logical AND):  if [ ! -e "$R1" ] && [ ! -e "$R2" ]. This condition is true only if both $R1 does not exist and $R2 does not exist. So, in the first case (||), the condition is true if at least one of the files does not exist, while in the second case (&&), the condition is true only if both files do not exist. While -a (logical and) and -o (logical or) are valid logical operators, && and || are more commonly used and are generally preferred for clarity and readability. They are also more widely supported and portable across different shells.  file="file.txt". ! is the logical NOT operator. It is used to negate the truth value of a condition. When placed in front of a command or condition, it reverses the logical sense of that condition. Conditional Expressions. Modern Bash syntax for conditional expressions encases our comparative expression inside double square brackets ([[ and ]]). In the examples below, we’ll show the syntax of different types of conditional expression. It’s worth noting that you won’t be able to run these examples on the command line directly as we’re just showing you the syntax. Don’t worry, you’ll be able to build on these in the task at the end of the section!. The syntax for this is: [[ option arg1 ]].  or. [[ arg1 operator arg2 ]]. A conditional expression returns a Boolean value i.e.  true or false. If the condition is met, it will return true and if not, false. It’s worth noting that the spacing is important. Here are some examples of valid and invalid conditional expression syntax. Valid: [[ -f ${file} ]]. Invalid: [[ -e file]]. [[-e file]]. [[-efile]]. File and variable operators. When we process files in our Bash scripts, it is often useful to check that they exist or whether they’re empty before the rest of our script proceeds. File operators allow us to perform checks on files and give us the opportunity to handle errors gracefully. Below are some of the most commonly used file operators. Returns true if the file exists: [[ -e ${file} ]]. Returns true if the file exists and is a directory: [[ -d ${directory} ]]. Returns true if the file exists and is a regular file: [[ -f ${file} ]]. Returns true if the file exists and is readable: [[ -r ${file} ]]. Returns true if the file exists and has a file size > 0: [[ -s ${file} ]]. We can also use conditional expressions to perform sanity checks on our variables. For example, checking whether a value has been assigned to a particular variable (e.g.  var): [[ -v ${var} ]]. Or, to check that the variable length is greater than 0: [[ -n ${string} ]]. Or, that the length of the variable is 0: [[ -z ${string} ]]. String comparisons. Strings, as sequences of characters, can be compared. There are two string conditional expressions you need to be aware of: Is equal to ==. Is not equal to !=. This condition will return true if string1 and string2 are identical: [[ ${string1} == ${string2} ]]. This condition will return true if string1 and string2 are different from one another: [[ ${string1} != ${string2} ]]. Arithmetic comparisons. In Bash, we don’t use the same syntax for string and arithmetic comparisons. There are six main arithmetic expressions: Is equal to -eq. Is not equal to -ne. Is less than -lt. Is less than or equal to -le. Is greater than -gt. Is greater than or equal to -ge. This condition will return true if arg1 is equal to arg2: [[ ${arg1} -eq ${arg2} ]]. Alternatively, this condition will return true if arg1 is not equal to arg2: [[ ${arg1} -ne ${arg2} ]]. This condition will return true if arg1 is less than arg2: [[ ${arg1} -lt ${arg2} ]]. Meanwhile, this condition will return true if arg1 less than or equal to arg2: [[ ${arg1} -le ${arg2} ]]. This condition will return true if arg1 is greater than arg2: [[ ${arg1} -gt ${arg2} ]]. And, finally, this condition will return true if arg1 is greater than or equal to arg2: [[ ${arg1} -ge ${arg2} ]]. Performing multiple comparisons. Using Bash syntax, we can also combine comparisons using the && and the || operators which represent AND and OR respectively. The following expression would only return true in the event that var1 is equal to var2 AND var3 is equal to var4: [[ ${var1} == ${var2} ]] && [[ ${var3} == ${var4} ]]. Alternatively, the following expression would only return true when either var1 is equal to var2 OR var3 is equal to var4: [[ ${var1} == ${var2} ]] || [[ ${var3} == ${var4} ]].  if [ "$var1" == "$var2" ]; then.  fi.  if [ "$var1" != "$var2" ]; then.  fi. SOLUTION. #!/usr/bin/env bash. # Store the first command line argument as a variable.  read -p "Please enter a diamond cut: " quality. # If quality is Fair or Good – return insufficient quality.  if [[ ${quality} == "Fair" ]] || [[ ${quality} == "Good" ]]; then.  echo "Insufficient quality to proceed".  elif [[ ${quality} != "Ideal" ]] && [[ ${quality} != "Very Good" ]] && [[ ${quality} != "Premium" ]] then. # If the cut is not a valid value echo. "Invalid cut".  else. # Using the -c option from grep to count.  grep -c "${quality}" Diamonds.csv.  fi. Let's check each of the conditions. (1) If a user enters Fair or Good, return Insufficient quality to proceed. ./diamonds.sh Please enter a diamond cut: Fair Insufficient quality to proceed Next, let's check that if we enter a value other than Premium, Ideal or Very Good, the script returns Invalid cut. ./diamonds.sh Please enter a diamond cut: Blargh Invalid cut Finally, let's see how many diamonds there are which are a Premium cut . /diamonds.sh Please enter a diamond cut: Premium 13791. CASE STATEMENTS. Sometimes, conditional logic may be too complex for if statements. In these situations, the issue isn’t that it’s impossible to write the logic as nested if statements, but that to do so could result in confusing code. In these situations, we can use case statements to check each of our conditions in turn and process commands based on those conditions. The case syntax looks like this:  case $string in.  pattern_1).  command. ;.  pattern_2).  alternate command. ;. *).  default command. ;.  esac. Let’s break this down. First, we start with case followed by the variable or expression we want to test and then in. Next, we have our case patterns against which we want to check our variable or expression. We use the ) symbol to signify the end of each pattern. After each pattern, you can then specify one or more commands you want to execute in the event that the pattern matches the expression or variable, terminating each clause with ;. As our last switch, it is common practice to have a default condition which is defined by having * as the pattern. Finally, we signify the end of our case statement by closing it with esac (case typed backwards!). Here is a simple example:  fruit="$1".  case $fruit in.  apple).  echo "Your apple will cost 35p". ;.  pear).  echo "Your pear will cost 41p". ;.  peach).  echo "Your peach will cost 50p". ;.  pineapple).  echo "Your pineapple will cost 75p". ;. *).  echo "Unknown fruit". ;.  esac.  animal=$1.  case $animal in.  cow).  echo "Here a moo". ;.  sheep).  echo "There a baa". ;.  duck).  echo "Everywhere a quack". ;. *).  echo "Old MacDonald had a farm". ;.  esac. FOR LOOP. What is a loop? A loop is a construct which allows you to repeatedly execute the same commands. We will be discussing three types of loops: for loops, while loops and until loops. Let’s start by looking at for loops. The basic syntax for a for loop is:  for variable in ${list}.  do.  done.  for n in {1. 3}.  do.  echo ${n}.  done.  for file in *.txt.  do.  echo ${file}.  done.  do.  echo $i.  done.  for (( i=1; i<=5; i++ )).  do.  if [ $i -eq 2 ].  then.  echo "fizz".  else.  echo "buzz".  fi.  done.  for (( i=1; i<=5; i++ )).  do.  if [[ $i -ne 2 ]].  then.  echo "buzz".  else.  echo "fizz".  fi.  done. While Loop. Both for loops and while loops are very similar. Typically, we use for loops where we know exactly how many iterations we need – i.e. , they have a definitive start and end point. On the other hand, while loops are used where we don’t know the limitations on tasks such as read in a file or asking a user for input. They just keep iterating as long as the specified condition has been met.  while [condition].  do.  done.  i=1.  while [[ $i -eq 1 ]].  do.  echo "hi".  done. This is what’s known as an infinite loop because the condition will always return true – i.e. , nothing is changing. In this example, “hi” will just keep being printed to the terminal until we force it to stop using Ctrl+C on our keyboard. So, that was how to use while loops in the wrong way. But, what do they look like when they are being used properly:  i=1.  while [[ $i -le 3 ]].  do.  echo "$i". (( i++ )).  done. Another common use for while loops is reading in the contents of a file. Here is an example:  while read data.  do.  echo "${data}".  done < infile.txt. This is what is known as a while loop. What do we mean by this? In this example, the while loop will only keep iterating while there are lines to be read from the given input file. Here, infile.txt is the name of the file that we are going to be looping over. The read command will process the file, line by line, into the data variable. Once it reaches the end of the file, the while loop will be terminated. < Redirects the contents of the file infile.txt as the input for the while loop. Each iteration of the loop reads a line from the file. The while loop starts with the condition and then repeats the command execution while the condition is valid. The condition is evaluated after each command execution. The do-while loop starts with the command execution and the condition is evaluated subsequently. Until Loop. The main difference is that while loops are designed to run while a condition is satisfied and then terminate once that condition returns false. On the other hand, until loops are designed to run while the condition returns false and only terminate when the condition returns true.  until [condition].  do.  done. For example, this loop would run until the variable is greater than 3:  i=1.  until [[ $i -gt 3 ]].  do.  echo $i. ((i++)).  done. This would output: 1.2.3. When you’re writing Bash scripts, you’ll often find that there are repetitive tasks. Instead of copying and pasting the same code to multiple places in your scripts, try using a function. Functions are a great way of producing reusable code! They are essentially a set of commands that can be called as many times as you need in your script. What’s even better is that functions are not unique to Bash, they’re a core component of many other programming languages too. Bash function syntax is pretty straightforward. We start off by defining the function name, followed by parentheses. The commands that we want to execute are found between the curly brackets and are known as the body of the function.  function my_function() {. #some code. }. There is an alternative syntax where you don’t have to prefix that first line with function:  my_function() {. #some code. }. However, it is much easier to pick out our functions if we use the previous syntax. It’s also a good idea to make sure that the names of your functions are relative and descriptive so that you can quickly see what they’re going to do. When we define a function, we are not executing it. Let’s use a simple toy example to demonstrate where we are using a function to return “Hello world” back to the terminal. We’ll call our function say_hello. You can see that we don’t execute the code in the function body until we specifically call (or execute) the function with say_hello. #!/usr/bin/env bash. # Define a function to print "hello world".  function say_hello() {.  echo "Hello world". }. # Execute the say_hello function.  say_hello. This would output: Hello world. We can adapt out function to take arguments using reserved variables. To access the first argument given to the function, we use the variable $1. Let’s tweak our script to use an argument, our name, that is provided to our say_hello function. #!/usr/bin/env bash. # Define a function to print "hello world".  function say_hello() {.  echo "Hello $1". }. # Execute the say_hello function.  say_hello "Victoria". This would output: Hello Victoria. Functions are a great way of producing reusable code!. Functions are one of the best ways to produce scalable and readable code. One general rule of thumb is not to make your functions too big. You can call a function within a function, so, break each function down into small, clear tasks. Your task. Create a function called file_exists taking the first argument (a filename) which it uses to see if the file exists. If it doesn’t, return “File does not exist: “, followed by the filename. Note: you can use the “!” notation when you want to check a negative. If file exists:  if [[ -e $1 ]]. If file does not exist:  if [[ ! -e $1 ]].  function file_exists() {.  if [[ ! -e $1 ]].  then.  echo "File does not exist: $1".  fi. }.  file_exists "wgt".  file="no_file.txt".  function file_exists() {.  if [[ ! -e $1 ]].  then.  echo "File does not exist: $1".  fi. }.  file_exists "${file}".  name="Victoria".  echo "Counting number of characters in name".  printf -- "${name}" | wc -m. -m specifically counts the number of characters.  printf command to print the value of the variable. ./script.sh 1>output.txt 2>error.txt. ./script.sh > combined_output.txt 2>&1. > combined_output.txt: Redirects both stdout and stderr to the file combined_output.txt. 2>&1: Redirects stderr to the same location as stdout. This command ensures that both regular output and error messages are captured in the same file (combined_output.txt). In Bash, <<, <, >, and >> are used for input and output redirection. They allow you to control where the input comes from and where the output goes. Here's a brief explanation of each: < (Input Redirection): This operator is used to take input from a file and provide it as input to a command.  command < input_file. For example, if you have a file named "input.txt" and you want to use it as input for a command called my_command, you can do: my_command < input.txt. > (Output Redirection): This operator is used to redirect the output of a command to a file. If the file already exists, it will be overwritten. command > output_file. For example, if you want to save the output of the echo command to a file named "output.txt," you can do:  echo "Hello, World!" > output.txt. This is particulary useful for big files. >> (Append Output Redirection): This operator is similar to >, but it appends the output to the end of the file rather than overwriting the file.  command >> output_file. For example, if you want to append the output of the echo command to a file named "output.txt," you can do:  echo "Appended text" >> output.txt. << (Here Document): This operator is used to pass a block of text as input to a command without the need for an external file. It's often used for providing multiple lines of input interactively.  command << EOL This is line 1. This is line 2. EOL. In this example, the text between << EOL and EOL is provided as input to the command. These redirection operators are fundamental for controlling input and output in Bash scripts and command-line operations. Top of Form. Try adding set -e to the top of your script: ensuring that the script will fail whenever an error occurs, no matter the exit code. If we want the script to exit with an error instead of continuing on silently, we can add the set -u command at the top of our script. Another default Bash behaviour is to only display results once a script has finished (set -x).  set -x command outputs the executed command before printing the command result.  set -eux combines all the checks. Track the Progress of Your Script and Redirect Script Outputs and Errors. In this step you will learn about some ways to track the progress of your script and to redirect script output to files. Tracking the progress of your script. Now, let us imagine you have a long and complex Bash script. You execute your script, it’s started running and you’ve gone off to make a cup of tea. Ten minutes later, you come back to check on its progress but, how do you know what’s going on and where you’ve gotten up to in your script?. There are many different ways in which we can track the progress of our scripts. The simplest is to break your script down into sections and output a progress statement when you start and/or finish each section. For example, let’s set our name as a variable and count the number of characters it contains. #!/usr/bin/env bash. # Set your name as a variable.  name="Victoria".  echo "Counting number of characters in name".  printf -- "${name}" | wc -m. As expected, we have our progress statement and the number of characters in our name: Counting number of characters in name. 8. Now, while this may seem excessive given the simple example, it’s clear that once we start to build up our scripts, adding progress statements will be invaluable. Particularly when were discussing loops this week, where it’s possible for your scripts to get stuck in an infinite loop, failing to exit. In those situations, progress statements are absolutely essential for debugging!. Discuss with your fellow learners: Can you see a situation where you would need to track the progress of your script?. Redirecting script outputs and errors. Despite your hardest efforts, sometimes your Bash scripts will do unexpected things. This is when we need to debug. If you have a long Bash script, it can be tricky to work out where things went wrong. To help with debugging, we can output progress statements at key points in our code e.g. “Reading in file: x”. However, these can easily fill up your terminal and become difficult to follow. A simple solution is to write these progress statements to one or more log files. Redirecting the output of scripts and commands to files. Simply put, redirection is the mechanism by which we can send the output of a command or script to another place. When we want to capture the output from a command or script, we usually choose to redirect those outputs into a file. To redirect the outputs of a script, we can use the > symbol:  script.sh > output.txt. Redirection using the > symbol works not only for scripts, but any Bash command:  echo "hello world" > hello.txt.  cat hello.txt.  hello world. Linux streams and file descriptors. Before we take an in depth look at how we redirect our outputs and errors to log files, we first need a crash course in Linux streams and file descriptors. These streams are handled like files – i.e.  you can read from them and you can write to them. There are three streams you should be aware of:  stdin (standard input).  stdout (standard output).  stderr (standard error). This sounds much more complicated than it really is. In a nutshell, stdout refers to the output from a command and stderr refers to the errors a command generates. The final stream, stdin refers to command line inputs which we’ll cover later in the week. Next, we need to understand file descriptors. A file descriptor is just a (positive) integer that represents an open file. Each of our Linux streams (i.e.  stdin, stdout and stderr) has been allocated a unique number in order to identify them. All you need to remember is which of the ids below corresponds to each of the streams: 0 => stdin. 1 => stdout. 2 => stderr. I/O redirection. To start understanding how these streams work, let’s look at redirecting the output from a script into a single file. Example script: #!/usr/bin/env bash. # A script that tries to change directory.  echo "Changing to a directory that doesn't exist".  cd foo. As you can see, our script returns the printed progress statement and an error that tells us that the directory we’re trying to migrate to doesn’t exist on our filesystem. ./script.sh. Changing to a directory that doesn't exist.  script.sh: line 6: cd: foo: No such file or directory. These two messages are being delivered to the terminal by two different Linux streams. The first message, our progress statement, is delivered via stdout. Meanwhile, the error message is delivered via stderr. Now, let’s see what happens when we try to redirect the outputs from that script into a file called output.txt: ./script.sh > output.txt. ./script.sh: line 6: cd: foo: No such file or directory. OK, so, we can see that the stdout has been redirected to our output file but, the error is still being displayed.  cat output.txt. Changing to a directory that doesn't exist. Why is this? Well, when we use > to redirect to a file, by default, the system will only redirect the stdout. But, what about our errors being delivered via stderr, how can we capture those?. To simplify things, let’s first look at how to redirect stdout and stderr to two different files. We’ll use the > symbol with our file descriptors (1 for stdout and 2 for stderr) to redirect our outputs to output.txt and our errors to error.txt respectively. ./script.sh 1>output.txt 2>error.txt. This command returns nothing back to our terminal. Using the cat command, we can see that, as expected, our outputs and errors have been written to output.txt and error.txt respectively. Our stdout (progress statement returned using echo): cat output.txt Changing to a directory that doesn’t exist. And our stderr (errors):  cat error.txt. ./script.sh: line 6: cd: foo: No such file or directory. In order to redirect the stdout and the stderr to the same place, we need to use a new term: 2>&1. When we use this, we redirect using the same syntax as before, but add 2>&1 to the end of our command. This is how it works in practice: ./script.sh > combined_output.txt 2>&1. Now, if we look at our combined output file, we can see that we’ve captured both the stdout and the stderr.  cat combined_output.txt. Changing to a directory that doesn't exist. ./script.sh: line 6: cd: foo: No such file or directory. Sometimes, despite having the very best intentions, subtle issues can creep into your script causing it to fail with unintended consequences. Fortunately, there are commands available to help with minimising these issues. One of these is the set command. Let’s take a look at how the set command can help us write robust and secure Bash scripts. First, how does the set command work? Using the set command allows us to customise the environment in which our scripts are run. The general syntax for the set command is:  set [options]. There are more than a dozen options available for the set command. To view them, you can run the following command:  help set. In this article, we’ll be focusing on the most commonly used options. Using set -e to catch errors. Sometimes, commands within your script may fail but, the downstream commands will continue to run. This can be extremely frustrating if you don’t see the error and assume that, as the script completed, everything has worked as expected. Here’s an example. First, we will try to change into a directory called foo and then list the contents of that foo directory. The key here is that the foo directory doesn’t actually exist so, we can’t get its contents. #!/usr/bin/env bash.  cd foo.  ls. What happens when we run our script?.  script.sh: line 3: cd: foo: No such file or directory. File1 File2. Notice that our script generated an error when the system couldn’t find our foo directory. But, because there wasn’t an exit code, the remaining commands in the script also ran. Unfortunately, this listed the contents of our current working directory and not the foo directory as intended. Imagine if this was part of a long series of output commands and we missed the error….we may accidentally assume that our script ran correctly!. Fortunately, the set -e command comes to our rescue by ensuring that the script will fail whenever an error occurs, no matter the exit code. Try adding set -e to the top of your script: #!/usr/bin/env bash.  set -e.  cd foo.  ls. Bingo! This time, we can see that the script terminates as soon as it reaches the first error. script.sh: line 5: cd:foo: No such file or directory. Using set -u to catch variables that don’t exist. By default, when executing a script, Bash will just ignore variables which don’t exist. In most cases, you won’t want this behaviour as it can have unexpected consequences!. In this example, we will first try to output a variable, $foo, which doesn’t exist and then try to output a simple string, bar. #!/usr/bin/env bash.  echo $foo.  echo bar. When we run this script, we get the following output:  bar. Notice that the system outputs a blank line for echo $foo. This is because Bash is ignoring $foo as it doesn’t exist. If we want the script to exit with an error instead of continuing on silently, we can add the set -u command at the top of our script. #!/usr/bin/env bash.  set -u.  echo $foo. echo bar. This will result in our script exiting with the following error:  script.sh: line 6:foo: unbound variable. Notice, our script terminates before running the second echo command. Displaying executed commands while script is running with set -x. Another default Bash behaviour is to only display results once a script has finished. This can be especially frustrating when you need to debug scripts that take a long time to run. Let’s take an example script that outputs two simple strings, foo and bar. #!/usr/bin/env bash.  echo foo.  echo bar. The output from this script would be:  foo.  bar. Now, what if we want to know which command is producing each of the results? To find this out, we can use the set -x command which outputs the executed command before printing the command result. #!/usr/bin/env bash.  set -x.  echo foo.  echo bar. Running this script would give the following output: + echo foo.  foo. + echo bar.  bar. As you can see, before executing each of the echo commands, the script first prints the command to the terminal, using a + to indicate that the output is a command. This can be especially handy when you want to debug long scripts. Combining set options in a single command. Most of the time, you will want to use all of these options together. Instead of writing the commands out, one command per line, we can combine the options into a single command:  set -eux. Using the set command is essential to building robust Bash scripts. Not only is it part of good scripting practices but, will also save you a lot of time and frustration!. “clean code”. Clean code is: Easy for someone to pick up and understand. Reusable. Scalable. Expects the unexpected. 1. Plan ahead. 2. Build your script in small steps. 3. Scale up slowly. 4. Comment, comment, comment. 5. Don’t prolong the life of the script unnecessarily (set options). 6. Keep on top of variable management (Variable names should be meaningful, use double quotes and curly braces to avoid issues with whitespace.  and wildcards in the variable value). 7. Prevent code bloat by using functions (Like variables, functions should be meaningfully named. They should be small, with a limited, clear task). 8. Don’t duplicate scripts (avoid hard coded paths). 9. Keep debugging simple (Scripts will fail, it’s inevitable). 10. Clean up after yourself (temporary files). 11. Make your code easy to read (Any fool can write code that a computer can understand. Good programmers write code that humans can understand). 12. Don’t walk away from new scripts (Sit back down and make sure it runs OK for the first couple of times). 13. Most of important of all, don’t be afraid to ask for help!. Each line in our sample data files is a biological unit of data known as an alignment. All we want to know is how many records (alignments) are in each of the data files. But, the catch here is that these files are not in a human readable format. This means we need to use a program, like samtools, to process them if we want to know the number of records (alignments) each file contains. Counting the number of alignments (amount of data) in a single file. So, to get the number of records (alignments) in our first file (sample_10000_11000.bam), we would run the following command:  samtools view -c sample_10000_11000.bam.  alignments=$(samtools view -c sample_10000_11000.bam). Now, if we echo our variable, you will see it has the expected value:  echo ${alignments}. 1947.  set -euxo pipefail.  set -e: Exit immediately if a command exits with a non-zero status.  set -u: Treat unset variables as an error.  set -x: Print each command and its arguments to the standard error output before executing it.  set -o pipefail: Causes a pipeline to produce a failure return code if any command in the pipeline fails.  for start in $(seq 10000 1000 12000); do. This loop uses the seq command to generate a sequence of numbers from 10000 to 12000 with an interval of 1000. The loop variable start takes each value in the sequence.  chr=1.  end=$((start + 1000)).  chr=1: Sets the chromosome number to 1.  end=$((start + 1000)): Calculates the end value for the range, which is start + 1000. Constructing Filename:  filename="sample_${start}_${end}.bam". Constructs an informative filename using the start and end values. Defining Data Source URL:  data_source=ftp://ftp. 1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00100/alignment/HG00100.mapped. ILLUMINA.bwa. GBR.low_coverage. 20130415.bam.cram. Specifies the URL of the example data file to be subset. Printing Information to User:  echo "Getting subset for ${filename} from: ${start} to: ${end}". Prints information about the subset being obtained. Using samtools to Get Subset:  samtools view -h -b -o "${filename}" "${data_source}" "${chr}:${start}-${end}".  samtools view: Invokes samtools to view or convert SAM/BAM/CRAM format. -h: Include header in output. -b: Output in the BAM format. -o "${filename}": Specifies the output file. "${data_source}" "${chr}:${start}-${end}": Specifies the input data source and the genomic region to subset. This script essentially iterates over specified ranges, constructs informative filenames,. and uses samtools to obtain subsets of a genomic data file for each range. The informative output and the use of variables make the script easy to understand and maintain. Note from the lecturer. One's tasks will always expand or shrink to fit in the time one gives them. df -h (check for available space). htop (check for cores and memory). shift alt position of ubuntu on the task bar (new terminal). If you've entered the R console from the command line and want to return to the command line without exiting R, you can use Ctrl + D (on Linux and macOS) or Ctrl + Z (on Windows) to signal the end of input, and you'll return to the command line. Ctrl shift 6 to set mark, arrows to move, shift tab to reduce indention or otherwise. Files and directories permissions. Linux is a multi-users OS. • On a Linux system, each file and directory is assigned access rights for the owner of the file, the members of a group of related users, and everybody else. Remember the ls -l example. Figure1015.1.3.shell_scripting_image_001.jpg. Permissions are broken into 4 sections. Figure1016.1.3.shell_scripting_image_002.jpg. Access permissions on files. r indicates read permission: the permission to read and and copy the file w indicates write permission: the permission to change a file x indicates execution permission: the permission to execute a file, where appropriate. Access permissions on directories. r indicates the permissions to list files in the directory. w indicates that users may delete files from the directory or move files into it. x indicates means the right to access files in the directory. This implies that you may read files in the directory provided you have read permission on the individual files. chmod command. Used to change the permissions of a file or a directory. Syntax: chmod options permissions filename. Only the owner of the file can use chmod to change the permissions. Permissions define permissions for the owner, the group of users and anyone else (others). There are two ways to specify the permissions: Symbols: alphanumeric characters. Octals: digits (0 to 7). Figure1017.1.3.shell_scripting_image_003.jpg. Octal permissions. 4 stands for "read”. 2 stands for "write”. 1 stands for "execute”. 0 stands for "no permission". chmod examples. chmod u=rwx,g=rx,o=r filename, same as chmod 754 filename. Figure1018.1.3.shell_scripting_image_004.jpg. More examples. 777: (rwxrwxrwx) No restrictions on permissions. Anybody may do anything. 755: (rwxrxrx) The file's owner may read, write, and execute the file. All others may read and execute the file (common for programs that are used by all users). 700: (rwx) The file's owner has all the rights. Nobody else has any rights (private for the owner). 666: (rwrwrw) All users may read and write the file. 644: (rwrr) The owner may read and write a file, while all others may only read the file (everybody may read, but only the owner may change). 600: (rw) The owner may read and write a file. All others have no rights. chown is a command in Unix-like operating systems used to change the ownership of files and directories. It allows you to specify new owners and/or groups for one or more files or directories. Key points about chown include: Changing Ownership: chown allows you to change the owner of a file or directory to a new user, as well as change the group ownership to a new group;

Syntax: The basic syntax of chown is chown [OPTIONS] OWNER[:GROUP] FILE..., where OWNER specifies the new owner, GROUP specifies the new group (optional), and FILE specifies the file(s) or directory(ies) to be changed;

Options: Some common options include -R for recursive operation (changing ownership of all files and directories within a directory), and -v for verbose output (displaying detailed information about the changes made);

Ownership Modifications: You can change ownership alone, both ownership and group, or just the group ownership. It's also possible to change ownership recursively for directories and their contents;

Caution: Use chown with caution, especially when using the -R option, as changing ownership recursively can affect many files and directories. Ensure that you specify the correct ownership to avoid unintended consequences;

Permissions: Changing ownership can impact file permissions, so ensure that permissions are appropriately set after changing ownership to maintain security and access control;

Overall, chown is a powerful command for managing file and directory ownership in Unix-like systems, allowing administrators to control access and manage resources effectively. Let's say you have a file named example.txt owned by the user olduser in the group oldgroup, and you want to change the owner to newuser and the group to newgroup. You would use the following command: chown newuser:newgroup example.txt. This command changes the owner of example.txt to newuser and the group to newgroup. If you only want to change the owner without changing the group, you can omit the :newgroup part:
chown newuser example.txt
This command changes the owner of example.txt to newuser while keeping the group unchanged. If you want to change the ownership of a directory and all its contents recursively, you can use the -R option: chown -R newuser:newgroup directory/. This command changes the owner of directory/ and all files and subdirectories within it to newuser and newgroup. Always make sure to double-check the ownership changes, especially when using the -R option, to avoid unintended modifications to your files and directories. Environment variables. Variables are areas of memory that can be used to store information and are referred to by a name. How to create a variable: a line that contains the name of the variable followed immediately by an equal sign ("="). 2 types of variables: shell variables and environment variables. Some variables are already set in your shell session. printenv: prints the values of all your environment variables. What is an environment variable? An environment variable is a dynamic "object" on a computer that stores a value, which in turn can be referenced by one or more programs. Environment variables help programs know what directory to install files in, where to store temporary files, where to find user profile settings, and other things. Environment variables help to create and shape the environment of where a program runs. Examples of environment variables. HOME: the environmental value that shows the current user's home directory, PATH: the environmental variable, which contains a colon-separated list of the directories that the system searches to find the executable program corresponding to a command issued by the user. PWD: always stores the value of your current working directory. Operators supported by shell. Figure1019.1.3.shell_scripting_image_005.jpg. Controlling tasks. Commands to control processes. ps: list the processes running on the system. kill: send a signal to one or more processes (usually to "kill" a process). jobs: an alternate way of listing your own processes. bg: put a process in the background. Launching a background job. Programs that take time or open a new Graphical User Interface. The prompt doesn’t reappear after the program is launched. The shell is waiting for the program to finish before control returns to you. ctlr+Z: interrupts a program Or You can put it in the background so that the prompt will return immediately; Use the command name followed by & to do so. SSH into a remote machine. What is SSH? SSH (secure Shell) is a protocol used to securely log onto remote systems (remote Linux machine and Unixlike servers). ssh command is the tool used in Linux to connect via SSH protocol. Syntax: ssh remoteusername@remotehost. Remote host could be an IP address or domain name. You will be asked to provide your password. To exit and go back to your local session, use exit. Multi-users in a Linux machine. While your computer only has one keyboard and monitor, it can still be used by more than on user. For example, if your computer is attached to a network, or the Internet, remote users can log in via ssh (secure shell) and operate the computer. Remote users can execute applications and have the output displayed on a remote computer. Copy files from or to a remote machine, scp: secure copy Syntax: scp pathfrom pathto. The difference: in scp, at least the source or the destination is in a remote machine Example: uploading all the .txt files from your current working directory to a remotehost scp ./*.txt username@myhost.com:/home/username/folder;